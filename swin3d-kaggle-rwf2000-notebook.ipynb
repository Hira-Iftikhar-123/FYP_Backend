{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4718786,"sourceType":"datasetVersion","datasetId":2730182}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === Cleaned & corrected notebook cell for Kaggle ===\n# Run this inside a Kaggle kernel (the dataset is already mounted at /kaggle/input/rwf2000)\n\n# 0. (Optional) Installs -- run only if you actually need them.\n# !pip install --quiet timm==0.6.13 einops decord==0.6.2\n\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# === 1. Paths (use the mounted dataset on Kaggle) ===\nBASE = \"/kaggle/input/rwf2000/RWF-2000\"  \nassert os.path.exists(BASE), f\"Base path not found: {BASE}\"\n\n# === 2. Build dataframe from existing train/val dirs (you already have train/val) ===\nrows = []\nfor split in [\"train\", \"val\"]:\n    for cls in [\"Fight\", \"NonFight\"]:\n        class_dir = os.path.join(BASE, split, cls)\n        if not os.path.exists(class_dir):\n            print(\"Missing folder:\", class_dir)\n            continue\n        videos = glob.glob(os.path.join(class_dir, \"*.avi\")) + glob.glob(os.path.join(class_dir, \"*.mp4\"))\n        for v in videos:\n            rows.append({\n                \"clip_path\": v,\n                \"label\": \"violence\" if cls == \"Fight\" else \"non_violence\",\n                \"split\": split\n            })\n\ndf = pd.DataFrame(rows)\nprint(\"Total videos found:\", len(df))\nprint(df['label'].value_counts())\n\n# === 3. OPTIONAL: Create train/val/test splits if you prefer new splits ===\n# If you want to keep the original train/val you can skip this section and create CSVs using df[df['split']=='train'] etc.\nuse_existing_splits = True\n\nif not use_existing_splits:\n    train_val, test = train_test_split(df, test_size=0.1, stratify=df[\"label\"], random_state=42)\n    train, val = train_test_split(train_val, test_size=0.12, stratify=train_val[\"label\"], random_state=42)\nelse:\n    train = df[df['split'] == 'train'].reset_index(drop=True)\n    val = df[df['split'] == 'val'].reset_index(drop=True)\n    # If there's no val in dataset and you want a test set, you can split train further.\n    test = pd.DataFrame(columns=df.columns)\n\n# Save CSVs to /kaggle/working (writable)\ntrain_csv = \"/kaggle/working/train.csv\"\nval_csv = \"/kaggle/working/val.csv\"\ntest_csv = \"/kaggle/working/test.csv\"\ntrain.to_csv(train_csv, index=False)\nval.to_csv(val_csv, index=False)\ntest.to_csv(test_csv, index=False)\nprint(\"Saved CSVs:\", train_csv, val_csv, test_csv)\nprint(\"Counts ->\", len(train), len(val), len(test))\n\n# === 4. Frame extraction util (decord preferred, fallback to cv2) ===\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\ntry:\n    from decord import VideoReader, cpu\n    decord_available = True\n    print(\"decord available\")\nexcept Exception as e:\n    decord_available = False\n    print(\"decord not available, will fallback to cv2:\", e)\n\ndef sample_frames_decord(vr, num_frames):\n    total = len(vr)\n    if total == 0:\n        return None\n    idxs = np.linspace(0, total - 1, num_frames).astype(int)\n    frames = vr.get_batch(idxs).asnumpy()  # (T,H,W,C)\n    return frames\n\ndef extract_frames(path, num_frames=16, target_size=(224,224)):\n    # Return (T,H,W,C) uint8 in RGB\n    if decord_available:\n        try:\n            vr = VideoReader(path, ctx=cpu(0))\n            frames = sample_frames_decord(vr, num_frames)\n            if frames is None or len(frames) < num_frames:\n                # pad by repeating last frame\n                if frames is None or len(frames) == 0:\n                    return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n                last = frames[-1]\n                pad = np.repeat(last[None, ...], num_frames - len(frames), axis=0)\n                frames = np.concatenate([frames, pad], axis=0)\n            # resize if needed (decord returns original sizes)\n            import cv2\n            resized = []\n            for f in frames:\n                resized.append(cv2.resize(f, target_size))\n            return np.stack(resized, axis=0)\n        except Exception as e:\n            # fallback\n            print(\"decord read error, falling back to cv2:\", e)\n\n    # cv2 fallback (safe)\n    import cv2\n    cap = cv2.VideoCapture(path)\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n    if total <= 0:\n        cap.release()\n        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n    idxs = np.linspace(0, total - 1, num_frames).astype(int)\n    frames = []\n    for idx in idxs:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame = cv2.resize(frame, target_size)\n        frames.append(frame)\n    cap.release()\n    if len(frames) < num_frames:\n        if len(frames) == 0:\n            return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n        last = frames[-1]\n        for _ in range(num_frames - len(frames)):\n            frames.append(last.copy())\n    return np.stack(frames, axis=0)\n\n# === 5. Dataset and transforms ===\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF\n\nframes_per_clip = 16\n\nclass VideoTransform:\n    def __init__(self, size=224):\n        self.size = size\n        self.resize = transforms.Resize((size, size))\n    def __call__(self, frames_np):\n        # frames_np: (T,H,W,C) uint8\n        T = frames_np.shape[0]\n        tensors = []\n        for i in range(T):\n            img = TF.to_pil_image(frames_np[i])\n            img = TF.resize(img, [self.size, self.size])\n            t = TF.to_tensor(img)  # C,H,W in [0,1]\n            tensors.append(t)\n        # stack into (C, T, H, W)\n        frames = torch.stack(tensors, dim=1)\n        mean = torch.tensor([0.485, 0.456, 0.406])[:, None, None, None]\n        std = torch.tensor([0.229, 0.224, 0.225])[:, None, None, None]\n        frames = (frames - mean) / std\n        return frames\n\nvideo_transform = VideoTransform(size=224)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(['non_violence', 'violence'])\n\nclass RWFVideoDataset(Dataset):\n    def __init__(self, csv_file, transform=None, frames_per_clip=16):\n        self.df = pd.read_csv(csv_file)\n        self.paths = self.df['clip_path'].values\n        self.labels = self.df['label'].values\n        self.transform = transform\n        self.frames_per_clip = frames_per_clip\n        self.le = le\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        label = self.labels[idx]\n        frames = extract_frames(p, num_frames=self.frames_per_clip, target_size=(224,224))\n        frames = self.transform(frames)  # (C,T,H,W)\n        label_idx = int(self.le.transform([label])[0])\n        return frames, label_idx\n\n# === 6. Model (use torchvision 3D model as backbone) ===\nimport torch.nn as nn\nimport torchvision\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntry:\n    from torchvision.models.video import mc3_18\n    backbone = mc3_18(pretrained=True)\n    # mc3_18 expects input (B,3,T,H,W) and has backbone.fc\n    backbone.fc = nn.Linear(backbone.fc.in_features, 2)\n    model = backbone.to(device)\n    print(\"Using mc3_18 backbone, device:\", device)\nexcept Exception as e:\n    print(\"Could not load mc3_18 pretrained, falling back to tiny 3D conv:\", e)\n    class Simple3D(nn.Module):\n        def __init__(self, num_classes=2):\n            super().__init__()\n            self.features = nn.Sequential(\n                nn.Conv3d(3, 32, kernel_size=(3,3,3), stride=1, padding=1),\n                nn.ReLU(),\n                nn.MaxPool3d((1,2,2)),\n                nn.Conv3d(32, 64, kernel_size=(3,3,3), padding=1),\n                nn.ReLU(),\n                nn.AdaptiveAvgPool3d((1,1,1)),\n            )\n            self.head = nn.Linear(64, num_classes)\n        def forward(self,x):\n            x = self.features(x)\n            x = x.view(x.size(0), -1)\n            return self.head(x)\n    model = Simple3D(num_classes=2).to(device)\n\n# === 7. Training setup ===\nfrom torch.optim import Adam\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscaler = torch.cuda.amp.GradScaler()\n\n# Dataloaders\nbatch_size = 2   # lower this if OOM\nnum_workers = 2\ntrain_dataset = RWFVideoDataset(train_csv, transform=video_transform, frames_per_clip=frames_per_clip)\nval_dataset = RWFVideoDataset(val_csv, transform=video_transform, frames_per_clip=frames_per_clip)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=(device=='cuda'))\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=(device=='cuda'))\n\n# === 8. Training loop (simple, mixed precision) ===\ndef train_one_epoch(model, loader, optimizer, criterion, device, scaler, epoch):\n    model.train()\n    running_loss = 0.0\n    total = 0\n    correct = 0\n    for i, (inputs, labels) in enumerate(loader):\n        inputs = inputs.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        # torchvision 3D expects (B,C,T,H,W) - our transform produces that\n        with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        running_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        total += labels.size(0)\n        correct += (preds == labels).sum().item()\n        if (i+1) % 50 == 0:\n            print(f'Epoch {epoch} Iter {i+1}/{len(loader)} Loss {running_loss/(i+1):.4f} Acc {correct/total:.4f}')\n    return running_loss / len(loader), correct / total\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            preds = outputs.argmax(dim=1)\n            total += labels.size(0)\n            correct += (preds == labels).sum().item()\n    return running_loss / len(loader), correct / total\n\n# === 9. Run a short training run (set epochs small for test) ===\nnum_epochs = 5\nbest_val_acc = 0.0\nfor epoch in range(1, num_epochs+1):\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler, epoch)\n    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n    print(f'Epoch {epoch} Train Loss {train_loss:.4f} Acc {train_acc:.4f} | Val Loss {val_loss:.4f} Acc {val_acc:.4f}')\n    # checkpoint (optional)\n    torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'optimizer': optimizer.state_dict()}, f'/kaggle/working/rwf_epoch{epoch}.pth')\n\nprint(\"Done.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:06:15.750781Z","iopub.execute_input":"2025-11-20T14:06:15.751531Z","iopub.status.idle":"2025-11-20T14:44:15.049522Z","shell.execute_reply.started":"2025-11-20T14:06:15.751495Z","shell.execute_reply":"2025-11-20T14:44:15.048728Z"}},"outputs":[{"name":"stdout","text":"Total videos found: 2000\nlabel\nviolence        1000\nnon_violence    1000\nName: count, dtype: int64\nSaved CSVs: /kaggle/working/train.csv /kaggle/working/val.csv /kaggle/working/test.csv\nCounts -> 1600 400 0\ndecord not available, will fallback to cv2: No module named 'decord'\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MC3_18_Weights.KINETICS400_V1`. You can also use `weights=MC3_18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Using mc3_18 backbone, device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/860627369.py:215: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n/tmp/ipykernel_48/860627369.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Iter 50/800 Loss 0.6433 Acc 0.6300\nEpoch 1 Iter 100/800 Loss 0.6438 Acc 0.6550\nEpoch 1 Iter 150/800 Loss 0.6440 Acc 0.6400\nEpoch 1 Iter 200/800 Loss 0.6352 Acc 0.6525\nEpoch 1 Iter 250/800 Loss 0.6305 Acc 0.6500\nEpoch 1 Iter 300/800 Loss 0.6217 Acc 0.6583\nEpoch 1 Iter 350/800 Loss 0.6188 Acc 0.6571\nEpoch 1 Iter 400/800 Loss 0.5959 Acc 0.6725\nEpoch 1 Iter 450/800 Loss 0.5929 Acc 0.6744\nEpoch 1 Iter 500/800 Loss 0.5831 Acc 0.6800\nEpoch 1 Iter 550/800 Loss 0.5797 Acc 0.6836\nEpoch 1 Iter 600/800 Loss 0.5817 Acc 0.6850\nEpoch 1 Iter 650/800 Loss 0.5797 Acc 0.6854\nEpoch 1 Iter 700/800 Loss 0.5787 Acc 0.6850\nEpoch 1 Iter 750/800 Loss 0.5762 Acc 0.6873\nEpoch 1 Iter 800/800 Loss 0.5672 Acc 0.6956\nEpoch 1 Train Loss 0.5672 Acc 0.6956 | Val Loss 0.3310 Acc 0.8550\nEpoch 2 Iter 50/800 Loss 0.4438 Acc 0.7700\nEpoch 2 Iter 100/800 Loss 0.4372 Acc 0.7800\nEpoch 2 Iter 150/800 Loss 0.4107 Acc 0.8033\nEpoch 2 Iter 200/800 Loss 0.4141 Acc 0.8025\nEpoch 2 Iter 250/800 Loss 0.4073 Acc 0.8120\nEpoch 2 Iter 300/800 Loss 0.4009 Acc 0.8150\nEpoch 2 Iter 350/800 Loss 0.3947 Acc 0.8186\nEpoch 2 Iter 400/800 Loss 0.3883 Acc 0.8225\nEpoch 2 Iter 450/800 Loss 0.3936 Acc 0.8189\nEpoch 2 Iter 500/800 Loss 0.3998 Acc 0.8200\nEpoch 2 Iter 550/800 Loss 0.3995 Acc 0.8191\nEpoch 2 Iter 600/800 Loss 0.3972 Acc 0.8217\nEpoch 2 Iter 650/800 Loss 0.3984 Acc 0.8208\nEpoch 2 Iter 700/800 Loss 0.4050 Acc 0.8179\nEpoch 2 Iter 750/800 Loss 0.4059 Acc 0.8173\nEpoch 2 Iter 800/800 Loss 0.4111 Acc 0.8119\nEpoch 2 Train Loss 0.4111 Acc 0.8119 | Val Loss 0.3487 Acc 0.8325\nEpoch 3 Iter 50/800 Loss 0.3422 Acc 0.8700\nEpoch 3 Iter 100/800 Loss 0.2990 Acc 0.8900\nEpoch 3 Iter 150/800 Loss 0.2985 Acc 0.8867\nEpoch 3 Iter 200/800 Loss 0.2992 Acc 0.8775\nEpoch 3 Iter 250/800 Loss 0.2964 Acc 0.8840\nEpoch 3 Iter 300/800 Loss 0.2969 Acc 0.8817\nEpoch 3 Iter 350/800 Loss 0.2939 Acc 0.8786\nEpoch 3 Iter 400/800 Loss 0.2925 Acc 0.8788\nEpoch 3 Iter 450/800 Loss 0.2951 Acc 0.8778\nEpoch 3 Iter 500/800 Loss 0.3067 Acc 0.8730\nEpoch 3 Iter 550/800 Loss 0.3095 Acc 0.8700\nEpoch 3 Iter 600/800 Loss 0.3104 Acc 0.8725\nEpoch 3 Iter 650/800 Loss 0.3091 Acc 0.8723\nEpoch 3 Iter 700/800 Loss 0.3176 Acc 0.8650\nEpoch 3 Iter 750/800 Loss 0.3232 Acc 0.8600\nEpoch 3 Iter 800/800 Loss 0.3202 Acc 0.8612\nEpoch 3 Train Loss 0.3202 Acc 0.8612 | Val Loss 0.3645 Acc 0.8725\nEpoch 4 Iter 50/800 Loss 0.2674 Acc 0.9100\nEpoch 4 Iter 100/800 Loss 0.2570 Acc 0.9150\nEpoch 4 Iter 150/800 Loss 0.2555 Acc 0.9133\nEpoch 4 Iter 200/800 Loss 0.2534 Acc 0.9075\nEpoch 4 Iter 250/800 Loss 0.2423 Acc 0.9080\nEpoch 4 Iter 300/800 Loss 0.2516 Acc 0.9050\nEpoch 4 Iter 350/800 Loss 0.2473 Acc 0.9086\nEpoch 4 Iter 400/800 Loss 0.2443 Acc 0.9087\nEpoch 4 Iter 450/800 Loss 0.2458 Acc 0.9100\nEpoch 4 Iter 500/800 Loss 0.2527 Acc 0.9070\nEpoch 4 Iter 550/800 Loss 0.2504 Acc 0.9064\nEpoch 4 Iter 600/800 Loss 0.2505 Acc 0.9083\nEpoch 4 Iter 650/800 Loss 0.2529 Acc 0.9069\nEpoch 4 Iter 700/800 Loss 0.2456 Acc 0.9107\nEpoch 4 Iter 750/800 Loss 0.2454 Acc 0.9100\nEpoch 4 Iter 800/800 Loss 0.2415 Acc 0.9125\nEpoch 4 Train Loss 0.2415 Acc 0.9125 | Val Loss 0.4655 Acc 0.7650\nEpoch 5 Iter 50/800 Loss 0.2061 Acc 0.9200\nEpoch 5 Iter 100/800 Loss 0.1977 Acc 0.9300\nEpoch 5 Iter 150/800 Loss 0.1859 Acc 0.9333\nEpoch 5 Iter 200/800 Loss 0.1723 Acc 0.9375\nEpoch 5 Iter 250/800 Loss 0.1511 Acc 0.9500\nEpoch 5 Iter 300/800 Loss 0.1498 Acc 0.9500\nEpoch 5 Iter 350/800 Loss 0.1620 Acc 0.9471\nEpoch 5 Iter 400/800 Loss 0.1535 Acc 0.9525\nEpoch 5 Iter 450/800 Loss 0.1599 Acc 0.9522\nEpoch 5 Iter 500/800 Loss 0.1663 Acc 0.9470\nEpoch 5 Iter 550/800 Loss 0.1671 Acc 0.9482\nEpoch 5 Iter 600/800 Loss 0.1666 Acc 0.9483\nEpoch 5 Iter 650/800 Loss 0.1667 Acc 0.9477\nEpoch 5 Iter 700/800 Loss 0.1719 Acc 0.9450\nEpoch 5 Iter 750/800 Loss 0.1726 Acc 0.9440\nEpoch 5 Iter 800/800 Loss 0.1716 Acc 0.9450\nEpoch 5 Train Loss 0.1716 Acc 0.9450 | Val Loss 0.3026 Acc 0.8700\nDone.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport glob\nfrom sklearn.model_selection import train_test_split\n\nBASE = \"/kaggle/input/rwf2000/RWF-2000\"\n\nrows = []\n\nfor split in [\"train\", \"val\"]:\n    split_dir = os.path.join(BASE, split)\n    \n    for class_name in [\"Fight\", \"NonFight\"]:\n        class_dir = os.path.join(split_dir, class_name)\n\n        # Match both AVI and MP4\n        videos = glob.glob(os.path.join(class_dir, \"*.avi\")) + \\\n                 glob.glob(os.path.join(class_dir, \"*.mp4\"))\n        \n        for v in videos:\n            rows.append({\n                \"clip_path\": v,\n                \"label\": \"violence\" if class_name == \"Fight\" else \"non_violence\"\n            })\n\ndf = pd.DataFrame(rows)\n\nprint(\"Total videos:\", len(df))\nprint(df.head())\nprint(df['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:48:25.797977Z","iopub.execute_input":"2025-11-20T13:48:25.798166Z","iopub.status.idle":"2025-11-20T13:48:28.877403Z","shell.execute_reply.started":"2025-11-20T13:48:25.798148Z","shell.execute_reply":"2025-11-20T13:48:28.876629Z"}},"outputs":[{"name":"stdout","text":"Total videos: 2000\n                                           clip_path     label\n0  /kaggle/input/rwf2000/RWF-2000/train/Fight/p1b...  violence\n1  /kaggle/input/rwf2000/RWF-2000/train/Fight/gHc...  violence\n2  /kaggle/input/rwf2000/RWF-2000/train/Fight/чЫС...  violence\n3  /kaggle/input/rwf2000/RWF-2000/train/Fight/XRC...  violence\n4  /kaggle/input/rwf2000/RWF-2000/train/Fight/4yT...  violence\nlabel\nviolence        1000\nnon_violence    1000\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":1}]}